import ArticleHero from "@/app/blog/components/ArticleHero";
import Info from "@/app/blog/components/Info";
import TableOfContents from "@/app/blog/components/TableOfContents";

export const metadata = {
	title: "10x Your Website Load Times",
	img: "",
	other: {
		created: "2025-03-15T21:44:00Z",
	}
};

<ArticleHero title={metadata.title} publishedAt={metadata?.other?.created} details={metadata?.details}>
	Optimize your website images with ffmpeg
</ArticleHero>

<div className="flex flex-row-reverse gap-28">
	<div className="sticky top-[calc(80px+2rem)] mt-20 h-fit grow">
		<TableOfContents headings={[
			{text: "Introduction", depth: 0},
			{text: "Downscaling images", depth: 0},
			{text: "Compressing images", depth: 0},
			{text: "Reducing the gap", depth: 0},
			]} />
	</div>

	<h2 id="introduction" className="visually-hidden">Introduction</h2>
	<div className="max-w-[70ch] pt-20">
		Picture this, you just finished creating a website for a friend, family, or client with amazing images of their work plastered around the beautiful pages you've created.
		You send them the url and once they open it up... they think they have internet issues. They try out any other website and everything just works, so why is their website loading so sloooooooow.

		That's exactly what I was wondering a while back for a website I built. Page load times were abysmal, and oh man I hated it.
		Probably because I was hosting everything on my "fancy" raspberry pi 4 with barely enough memory (2GB btw) to survive the node_modules cataclysm. Maybe it was because of my tragic network upload speeds of 10mbps. This speed didn't play nicely with the amount of data I needed to send over the wire for my website to work.
		The index alone contained a total of 23mb worth of content (js, images, video) to load.

		I'll give you a list of tricks that I used to make it work even on your local machine.

		## SSG

		I wanted:
		- Easy development
		- Fast testing
		- Ability to write highly reactive components

		without:
		- Too much complexity and overhead

		I ended up hating (React + Vite) it as it needed constant configuration and optimization

		NextJS gave me SSG and some extra coolness that I'd have to write myself anyways

		## Lazy loading content

		## Prefetching

		## Downscaling images

		You know your screen cannot show more pixels than it has right?

		All device screens, from smartphones to desktop monitors, have a fixed number of pixels to show images to your eyes.
		This means that if we try to squeeze an image that is small enough 

		[Image resizing in browsers](https://stackoverflow.com/a/31078909)

		Let's look at an example:
		\<show image with high res and low res side by side in a 320p container\>

		You can see that these are perceptually almost identical... if you have a keen eye of course you'll see a slight difference and that's because they're different sizes.

		But the difference isn't at all worsening our user's experience and is saving us ~87.7% bandwith, nice!

		Now, the way we achieve this is pretty straightforward. Using ffmpeg we downscale our images like so:
		```shell
		ffmpeg -i <input> -vf scale=<width>:-1 <output>
		```

		Where:
		- `input` &mdash;&nbsp; the input file's name
		- `output` &mdash;&nbsp; the output file's name
		- `width` &mdash;&nbsp; the width of our image

		In the scale option -1 retains the aspect ratio of the image.

		Imagine having a user upload his profile icon which will be displayed in a 64x64 div but displaying there the original image which could be anything from a small icon up to a 1080p or 2k image... that would be stupendously inefficient.

		Instead, with this trick we can create variations of the original for any situation.

		## Compressing images

		But wait, There's more!
		These images we were using so far are stored in a lossless format. What does this mean exactly?
		This means that compared to the real, or digital world these formats don't lose any quality making them store significantly more data to capture everything. Which is useful in some cases, but not in most.

		What we can do is convert any lossless image format to jpg or webp.
		This depending on the image will significantly drop the image size and if done right we can have it get really close to the identical.

		Some snippets that achieve this are the following:
		...

		As you can see the conversion to a lossy version is further reducing the size and helping us A TON. But what should we do first, compress and then downscale or downscale and then compress?
		Honestly, some people suggest compressing first and then resizing others the exact opposite as seen [here](https://stackoverflow.com/a/12012801). I would recommend to experiment and find out what works best for the current situation.

		<Info title="A note on GZIP">
		It's generally not recommended to gzip lossy images as they're already compressed enough. Quite possibly it might work agianst your intentions and increase the file size instead of reducing it. That'll happen because of our images already being compressed.
		</Info>

		## Reducing the gap

		So far we've seen how to manipulate our images to improve their load times, what if I could tell you we can improve load times regardless of our server setup, network speed or location.

		Imagine this, you've just ordered a pizza from two different shops, one is at the end of the street you live and the other on the other side of the city. Which one would reach you faster?
		Exactly! Keeping our information close to our users helps it reach faster to them. This is made possible using a Content Delivery Network (CDN) which has servers distributed across multiple locations (hopefully the entire globe) to get our data as close as we can to our users.

		The added benefit is that they also provide caching for frequently used data, meaning that the more requests we see from our users, the faster it will be for these services to retireve and show it to them.
		I'm currently using Cloudflare to store our images and provide us the sweet benefits of caching but we can definitely use any other CDN available online.

		I'll upload my information to Cloudflare's object store, which can be done for free once you have a domain name, and have it available anywhere online. For anyone following the process is as simple as
		...

		And now we can see it using the following url:
		...

		As you can definitely see these steps quickly add up, but your user's experience will guaranteed be way snappier than before.

		Something that you should consider is automating this task with a script or even an entire API if necessary to manage this process. I've created a quick and simple API that converts, downscales and uploads these images to cloudflare using their AWS S3 compatibe SDK which on the cloudflare docs was a little outdated and misleading at times. If you want to set something similar up yourself, feel free to do so by looking at my code [here]()
	</div>
</div>
